{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Motivating--The-Bayesian-Classifier-游븷\" data-toc-modified-id=\"Motivating--The-Bayesian-Classifier-游븷-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Motivating  The Bayesian Classifier 游븷</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes-Setup\" data-toc-modified-id=\"Naive-Bayes-Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Naive Bayes Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-So-Great-About-This?\" data-toc-modified-id=\"What's-So-Great-About-This?-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>What's So Great About This?</a></span></li></ul></li><li><span><a href=\"#The-Naive-Assumption\" data-toc-modified-id=\"The-Naive-Assumption-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Naive Assumption</a></span></li><li><span><a href=\"#The-Formula\" data-toc-modified-id=\"The-Formula-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>The Formula</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-Parts-Can-We-Find?\" data-toc-modified-id=\"What-Parts-Can-We-Find?-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>What Parts Can We Find?</a></span></li></ul></li><li><span><a href=\"#Calculating-the-Probability-That-Our-Email-Is-Spam\" data-toc-modified-id=\"Calculating-the-Probability-That-Our-Email-Is-Spam-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Calculating the Probability That Our Email Is Spam</a></span></li><li><span><a href=\"#Extending-It-With-Multiple-Words\" data-toc-modified-id=\"Extending-It-With-Multiple-Words-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Extending It With Multiple Words</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes-Modeling-Example\" data-toc-modified-id=\"Naive-Bayes-Modeling-Example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Naive Bayes Modeling Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Bayes's-Theorem-for-Classification\" data-toc-modified-id=\"Using-Bayes's-Theorem-for-Classification-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Using Bayes's Theorem for Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Does-this-look-like-a-classification-problem?\" data-toc-modified-id=\"Does-this-look-like-a-classification-problem?-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Does this look like a classification problem?</a></span></li></ul></li><li><span><a href=\"#Elephant-Example\" data-toc-modified-id=\"Elephant-Example-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Elephant Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes-by-Hand\" data-toc-modified-id=\"Naive-Bayes-by-Hand-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Naive Bayes by Hand</a></span></li><li><span><a href=\"#Priors\" data-toc-modified-id=\"Priors-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Priors</a></span></li><li><span><a href=\"#Calculation-of-Likelihoods\" data-toc-modified-id=\"Calculation-of-Likelihoods-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Calculation of Likelihoods</a></span></li><li><span><a href=\"#Posteriors\" data-toc-modified-id=\"Posteriors-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Posteriors</a></span></li><li><span><a href=\"#More-Dimensions\" data-toc-modified-id=\"More-Dimensions-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>More Dimensions</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-&quot;Naive&quot;-about-This?\" data-toc-modified-id=\"What's-&quot;Naive&quot;-about-This?-3.2.5.1\"><span class=\"toc-item-num\">3.2.5.1&nbsp;&nbsp;</span>What's \"Naive\" about This?</a></span></li><li><span><a href=\"#Priors\" data-toc-modified-id=\"Priors-3.2.5.2\"><span class=\"toc-item-num\">3.2.5.2&nbsp;&nbsp;</span>Priors</a></span></li><li><span><a href=\"#Likelihoods\" data-toc-modified-id=\"Likelihoods-3.2.5.3\"><span class=\"toc-item-num\">3.2.5.3&nbsp;&nbsp;</span>Likelihoods</a></span></li><li><span><a href=\"#Posteriors\" data-toc-modified-id=\"Posteriors-3.2.5.4\"><span class=\"toc-item-num\">3.2.5.4&nbsp;&nbsp;</span>Posteriors</a></span></li></ul></li><li><span><a href=\"#GaussianNB\" data-toc-modified-id=\"GaussianNB-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\" target=\"_blank\"><code>GaussianNB</code></a></a></span></li></ul></li><li><span><a href=\"#Comma-Survey-Example\" data-toc-modified-id=\"Comma-Survey-Example-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Comma Survey Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-Priors-and-Likelihoods\" data-toc-modified-id=\"Calculating-Priors-and-Likelihoods-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Calculating Priors and Likelihoods</a></span></li><li><span><a href=\"#Calculating-Posteriors\" data-toc-modified-id=\"Calculating-Posteriors-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Calculating Posteriors</a></span></li><li><span><a href=\"#Comparison-with-MultinomialNB\" data-toc-modified-id=\"Comparison-with-MultinomialNB-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Comparison with <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\" target=\"_blank\"><code>MultinomialNB</code></a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.823506Z",
     "start_time": "2022-08-16T17:15:31.363947Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "    # There is also a BernoulliNB for a dataset with binary predictors\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Describe how Bayes's Theorem can be used to make predictions of a target\n",
    "- Identify the appropriate variant of Naive Bayes models for a particular business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivating  The Bayesian Classifier 游븷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Let's take a second to go through an example to get a feel for how Bayes' Theorem can help us with classification. Specifically about document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam, Spam, Spam, Spam, Spam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Many cans of spam](images/wall_of_spam.jpeg)\n",
    "\n",
    "> This is the classic example: detecting email spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The Problem Setup**\n",
    "\n",
    "> We get emails that can be either emails we care about (***ham*** 游냥) or emails we don't care about (***spam*** 游볾). \n",
    ">\n",
    "> We can probably look at the words in the email and get an idea of whether they are spam or not just by observing if they contain red-flag words 游뛀\n",
    "> \n",
    "> We won't always be right, but if we see an email that uses word(s) that are more often associated with spam, then we can feel more confident as labeling that email as spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Naive Bayes Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we gotta do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Look at spam and not spam (ham) emails\n",
    "2. Identify words that suggest classification\n",
    "3. Determine probability that words occur in each classification\n",
    "4. Profit (classify new emails as \"spam\" or \"ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What's So Great About This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can keep updating our belief based on the emails we detect\n",
    "- Relatively simple\n",
    "- Can expand to multiple words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Naive Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "\n",
    "In practice, makes sense & is usually pretty good assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say the word that occurs is \"cash\":\n",
    "\n",
    "$$ P(游볾 | \"cash\") = \\frac{P(\"cash\" | 游볾)P(游볾)}{P(\"cash\")}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What Parts Can We Find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $P(\"cash\")$\n",
    "    * That's just the probability of finding the word \"cash\"! Frequency of the word!\n",
    "- $P(游볾)$\n",
    "    * Well, we start with some data (*prior knowledge*): The frequency of the spam occurring!\n",
    "- $P(\"cash\" | 游볾)$\n",
    "    * How frequently \"cash\" is used in known spam emails. Count the frequency across all spam emails\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Calculating the Probability That Our Email Is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.838945Z",
     "start_time": "2022-08-16T17:15:32.824949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's just say 2% of all emails have the word \"cash\" in them\n",
    "p_cash = 0.02\n",
    "\n",
    "# We normally would measure this from our data, but we'll take \n",
    "# it that 10% of all emails we collected were spam\n",
    "p_spam = 0.10\n",
    "\n",
    "# 12% of all spam emails have the word \"cash\"\n",
    "p_cash_given_its_spam = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.854445Z",
     "start_time": "2022-08-16T17:15:32.840948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_spam_given_cash = p_cash_given_its_spam * p_spam / p_cash\n",
    "\n",
    "print(f'If the email has the word \"cash\" in it, there is a {p_spam_given_cash*100}% chance the email is spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Check it**: Does this make sense? <br>\n",
    "> Suppose I had 250 total emails.\n",
    "> - How many should I expect to have the word 'cash' in them?\n",
    "> - How many should I expect to be spam?\n",
    "> - How many *of the spam emails* should I expect to have the word 'cash' in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:11:38.861341Z",
     "start_time": "2022-08-16T17:11:38.853342Z"
    },
    "hidden": true
   },
   "source": [
    "<details>\n",
    "    <summary> Answers </summary>\n",
    "    \n",
    "    5\n",
    "    25\n",
    "    3\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extending It With Multiple Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> With more words, the more certain we can be if it is/isn't spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam:\n",
    "\n",
    "$$ P(游볾\\ |\"buy\",\\ \"cash\") \\propto P(\"buy\",\\ \"cash\"|\\ 游볾)\\ P(游볾)$$\n",
    "\n",
    "\n",
    "But because of independence: \n",
    "    \n",
    "$$ P(\"buy\",\\ \"cash\"|\\ 游볾) = P(\"buy\"|\\ 游볾)\\ P(\"cash\"|\\ 游볾)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Normalize by dividing!\n",
    "\n",
    "$$\n",
    "P(游볾\\ |\"buy\",\\ \"cash\")  =\n",
    "    \\frac\n",
    "        {P(\"buy\"|\\ 游볾)P(\"cash\"|\\ 游볾)\\ P(游볾)}\n",
    "        {P(\"buy\"|\\ 游볾)P(\"cash\"|\\ 游볾)\\ P(游볾) + P(\"buy\"|\\ 游냥)P(\"cash\"|\\ 游냥)\\ P(游냥)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Note:** If we wanted to find the most probable class (especially useful for *multiclass*), we find the maximum numerator for the given criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Naive Bayes Modeling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using Bayes's Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's recall Bayes's Theorem:\n",
    "\n",
    "$\\large P(h|e) = \\frac{P(h)P(e|h)}{P(e)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Does this look like a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Suppose we have three competing hypotheses $\\{h_1, h_2, h_3\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $P(h_1|e) = \\frac{P(h_1)P(e|h_1)}{P(e)}$\n",
    "        - $P(h_2|e) = \\frac{P(h_2)P(e|h_2)}{P(e)}$\n",
    "        - $P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        \n",
    "- Suppose the evidence is a collection of elephant heights.\n",
    "- Suppose each of the three hypotheses claims that the elephant whose measurements we have belongs to one of the three extant elephant species (*L. africana*, *L. cyclotis*, and *E. maximus*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In that case the left-hand sides of these equations represent the probability that the elephant in question belongs to a given species.\n",
    "\n",
    "If we think of the species as our target, then **this is just an ordinary classification problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What about the right-hand sides of the equations? **These other probabilities we can calculate from our dataset.**\n",
    "\n",
    "- The priors can simply be taken to be the percentages of the different classes in the dataset.\n",
    "- What about the likelihoods?\n",
    "    - If the relevant features are **categorical**, we can simply count the numbers of each category in the dataset. For example, if the features are whether the elephant has tusks or not, then, to calculate the likelihoods, we'll just count the tusked and non-tuksed elephants per species.\n",
    "    - If the relevant features are **numerical**, we'll have to do something else. A good way of proceeding is to rely on (presumed) underlying distributions of the data. [Here](https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f) is an example of using the normal distribution to calculate likelihoods. We'll follow this idea below for our elephant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Elephant Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.869946Z",
     "start_time": "2022-08-16T17:15:32.856448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs = pd.read_csv('data/elephants.csv', usecols=['height (cm)',\n",
    "                                                   'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.885447Z",
     "start_time": "2022-08-16T17:15:32.871489Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:32.900992Z",
     "start_time": "2022-08-16T17:15:32.886946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.148975Z",
     "start_time": "2022-08-16T17:15:32.902480Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'maximus']['height (cm)'],\n",
    "            ax=ax, label='maximus')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'africana']['height (cm)'],\n",
    "            ax=ax, label='africana')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'cyclotis']['height (cm)'],\n",
    "            ax=ax, label='cyclotis')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Naive Bayes by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.164445Z",
     "start_time": "2022-08-16T17:15:33.151446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(elephs, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make prediction of species for some new elephant whose height we've just recorded. We'll suppose the new elephant has a height of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.179945Z",
     "start_time": "2022-08-16T17:15:33.165946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_pt = test.tail(1)\n",
    "new_ht = test_pt['height (cm)'].values[0]\n",
    "new_ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we want to calculate is the mean and standard deviation for height for each elephant species in our training data. We'll use these to calculate the relevant likelihoods.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.195445Z",
     "start_time": "2022-08-16T17:15:33.181445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_stats = train[train['species'] == 'maximus'].describe().loc[['mean', 'std'], :]\n",
    "max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.210946Z",
     "start_time": "2022-08-16T17:15:33.196446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_stats = train[train['species'] == 'cyclotis'].describe().loc[['mean', 'std'], :]\n",
    "cyc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.226446Z",
     "start_time": "2022-08-16T17:15:33.212446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_stats = train[train['species'] == 'africana'].describe().loc[['mean', 'std'], :]\n",
    "afr_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$Posterior = \\frac{Likelihood * Prior}{Marginalization}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The priors we just take directly from the distribution in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.242447Z",
     "start_time": "2022-08-16T17:15:33.227947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.257445Z",
     "start_time": "2022-08-16T17:15:33.245946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_prior = len(train[ train['species'] == 'maximus']) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.272945Z",
     "start_time": "2022-08-16T17:15:33.258946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_prior = len(train[ train['species'] == 'cyclotis']) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.288445Z",
     "start_time": "2022-08-16T17:15:33.274446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_prior = len(train[ train['species'] == 'africana']) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculation of Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use the PDFs of the normal distributions with the discovered means and standard deviations to calculate likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.303945Z",
     "start_time": "2022-08-16T17:15:33.289945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_likelihood = stats.norm(loc=max_stats['height (cm)'][0],\n",
    "           scale=max_stats['height (cm)'][1]).pdf(new_ht)\n",
    "max_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.319445Z",
     "start_time": "2022-08-16T17:15:33.304945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_likelihood = stats.norm(loc=cyc_stats['height (cm)'][0],\n",
    "          scale=cyc_stats['height (cm)'][1]).pdf(new_ht)\n",
    "cyc_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.334945Z",
     "start_time": "2022-08-16T17:15:33.320946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_likelihood = stats.norm(loc=afr_stats['height (cm)'][0],\n",
    "          scale=afr_stats['height (cm)'][1]).pdf(new_ht)\n",
    "afr_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we have just calculated are (approximations of) the likelihoods, i.e.:\n",
    "\n",
    "- $P(height=263 | species=maximus) = 1.51\\%$\n",
    "- $P(height=263 | species=cyclotis) = 2.36\\%$\n",
    "- $P(height=263 | species=africana) = 1.57\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Notice that they do NOT sum to 1!) But what we'd really like to know are the posteriors. I.e. what are:\n",
    "\n",
    "- $P(species=maximus | height=263)$?\n",
    "- $P(species=cyclotis | height=263)$?\n",
    "- $P(species=africana | height=263)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get these we can simply apply Bayes's Theorem! Now our priors (based on our training data) are $\\frac{39}{112}$ for both maximus and cyclotis and $\\frac{34}{112}$ for africana.\n",
    "\n",
    "So we can calculate the probability of the evidence (the denominator in Bayes's Theorem) as:\n",
    "\n",
    "$P(height=263) = \\frac{39}{112}(0.0151 + 0.0236) + \\frac{34}{112}(0.0157) = 0.0183$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.350446Z",
     "start_time": "2022-08-16T17:15:33.336446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prob_evidence = (max_prior * max_likelihood) + (cyc_prior * cyc_likelihood) + (afr_prior * afr_likelihood)\n",
    "prob_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And therefore calculate the posteriors using Bayes's Theorem:\n",
    "\n",
    "- $P(species=maximus | height=263) = \\frac{39}{112}\\frac{0.0151}{0.0183} = 28.9\\%$;\n",
    "- $P(species=cyclotis | height=263) = \\frac{39}{112}\\frac{0.0236}{0.0183} = 45.0\\%$;\n",
    "- $P(species=africana | height=263) = \\frac{34}{112}\\frac{0.0157}{0.0183} = 26.1\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.365947Z",
     "start_time": "2022-08-16T17:15:33.351945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(max_prior * max_likelihood / prob_evidence)\n",
    "print(cyc_prior * cyc_likelihood / prob_evidence)\n",
    "print(afr_prior * afr_likelihood / prob_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.381496Z",
     "start_time": "2022-08-16T17:15:33.367446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(max_prior * max_likelihood / prob_evidence) + (cyc_prior * cyc_likelihood / prob_evidence) +(afr_prior * afr_likelihood / prob_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bayes's Theorem shows us that the largest posterior belongs to the *cyclotis* species.\n",
    "\n",
    "Therefore, **the *cyclotis* species will be our prediction** for an elephant of this height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Is that prediction correct? Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.396946Z",
     "start_time": "2022-08-16T17:15:33.385446Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pt['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Turns out we were dealing with a shorter-than-average member of the *africana* species rather than a more typical member of the *cyclotis* species.\n",
    "\n",
    "Let's see if we can get a better model by adding more complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### More Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In fact, we also have elephant *weight* data available in addition to their heights. To accommodate multiple features we can make use of **multivariate normal** distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![multivariate-normal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/440px-MultivariateNormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### What's \"Naive\" about This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For multiple predictors, we make the simplifying assumption that **our predictors are probablistically independent**. This will often be unrealistic, but it simplifies our calculations a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.412477Z",
     "start_time": "2022-08-16T17:15:33.399946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants = pd.read_csv('data/elephants.csv',\n",
    "                       usecols=['height (cm)', 'weight (lbs)', 'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.427976Z",
     "start_time": "2022-08-16T17:15:33.413946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.443476Z",
     "start_time": "2022-08-16T17:15:33.429446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "two_dim_train, two_dim_test = train_test_split(elephants, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.458977Z",
     "start_time": "2022-08-16T17:15:33.444945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maximus = two_dim_train[two_dim_train['species'] == 'maximus']\n",
    "cyclotis = two_dim_train[two_dim_train['species'] == 'cyclotis']\n",
    "africana = two_dim_train[two_dim_train['species'] == 'africana']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose our new elephant with a height of 272 cm also has a weight of 8590 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.474445Z",
     "start_time": "2022-08-16T17:15:33.460445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_pt_2 = two_dim_test.tail(1)\n",
    "\n",
    "test_ht, test_wt = test_pt_2['height (cm)'].values[0], test_pt_2['weight (lbs)'].values[0]\n",
    "test_ht, test_wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.489979Z",
     "start_time": "2022-08-16T17:15:33.475946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "two_dim_train['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.505471Z",
     "start_time": "2022-08-16T17:15:33.491446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max__prior = len(train[ train['species'] == 'maximus']) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.520972Z",
     "start_time": "2022-08-16T17:15:33.506946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_prior = len(train[ train['species'] == 'cyclotis']) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.536446Z",
     "start_time": "2022-08-16T17:15:33.522477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_prior = len(train[ train['species'] == 'africana']) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.551972Z",
     "start_time": "2022-08-16T17:15:33.537945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_max = stats.multivariate_normal(mean=maximus.mean(),\n",
    "                          cov=maximus.cov()).pdf([test_ht, test_wt])\n",
    "likeli_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.567448Z",
     "start_time": "2022-08-16T17:15:33.553445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_cyc = stats.multivariate_normal(mean=cyclotis.mean(),\n",
    "                         cov=cyclotis.cov()).pdf([test_ht, test_wt])\n",
    "likeli_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.582971Z",
     "start_time": "2022-08-16T17:15:33.568946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_afr = stats.multivariate_normal(mean=africana.mean(),\n",
    "                         cov=africana.cov()).pdf([test_ht, test_wt])\n",
    "likeli_afr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.598446Z",
     "start_time": "2022-08-16T17:15:33.584448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "post_max = likeli_max / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_cyc = likeli_cyc / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_afr = likeli_afr / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "\n",
    "print(post_max)\n",
    "print(post_cyc)\n",
    "print(post_afr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.613976Z",
     "start_time": "2022-08-16T17:15:33.599946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note that this sets priors based on the training data\n",
    "# distribution by default!\n",
    "\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.629447Z",
     "start_time": "2022-08-16T17:15:33.615946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = two_dim_train.drop('species', axis=1)\n",
    "y = two_dim_train['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.644945Z",
     "start_time": "2022-08-16T17:15:33.630945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.660496Z",
     "start_time": "2022-08-16T17:15:33.646446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([test_ht, test_wt]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Did we get it right this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.675977Z",
     "start_time": "2022-08-16T17:15:33.661948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.predict(np.array([test_ht, test_wt]).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.691445Z",
     "start_time": "2022-08-16T17:15:33.677446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_pt_2['species'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All right! Note that the class names are arranged alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.706976Z",
     "start_time": "2022-08-16T17:15:33.692945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check the model's accuracy on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.722475Z",
     "start_time": "2022-08-16T17:15:33.708446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = two_dim_test.drop('species', axis=1), two_dim_test['species']\n",
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What about the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.908476Z",
     "start_time": "2022-08-16T17:15:33.724445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Comma Survey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.923945Z",
     "start_time": "2022-08-16T17:15:33.909945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.939445Z",
     "start_time": "2022-08-16T17:15:33.925445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.954945Z",
     "start_time": "2022-08-16T17:15:33.940946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll go ahead and drop the NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.970446Z",
     "start_time": "2022-08-16T17:15:33.956446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = commas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:33.985946Z",
     "start_time": "2022-08-16T17:15:33.971946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll call this our total dataset. Let's do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.001445Z",
     "start_time": "2022-08-16T17:15:33.987945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(commas, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first question on the survey was about the Oxford comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.016945Z",
     "start_time": "2022-08-16T17:15:34.002945Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train['Age']\n",
    "y_train = train['In your opinion, which sentence is more gramatically correct?']\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Personally, I like the Oxford comma, since it can help eliminate ambiguities, such as:\n",
    "\n",
    "\"This book is dedicated to my parents, Ayn Rand, and God\" <br/> vs. <br/>\n",
    "\"This book is dedicated to my parents, Ayn Rand and God\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how a Naive Bayes model would make a prediction here. We'll think of the comma preference as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.032447Z",
     "start_time": "2022-08-16T17:15:34.018446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make a prediction about Oxford comma usage for a new person who falls into the **45-60 age group**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Priors and Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following code makes a table of values that count up the number of survey respondents who fall into each of eight bins (the four age groups and the two answers to the first comma question). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.047947Z",
     "start_time": "2022-08-16T17:15:34.033946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = np.zeros((2, 4))\n",
    "\n",
    "for idx, value in enumerate(X_train.value_counts().index):\n",
    "    table[0, idx] = len(train[(train['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind, and loyal.') & (train['Age'] == value)])\n",
    "    table[1, idx] = len(train[(train['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind and loyal.') & (train['Age'] == value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.063445Z",
     "start_time": "2022-08-16T17:15:34.049946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Which age group is which?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.094445Z",
     "start_time": "2022-08-16T17:15:34.065446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.109945Z",
     "start_time": "2022-08-16T17:15:34.095946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table, columns=['Age45-60',\n",
    "                            'Age30-44',\n",
    "                            'Age>60',\n",
    "                            'Age18-29'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Which comma preference is which?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.125445Z",
     "start_time": "2022-08-16T17:15:34.111446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.140945Z",
     "start_time": "2022-08-16T17:15:34.126945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy = df_copy[['Age>60', 'Age45-60', 'Age30-44', 'Age18-29']]\n",
    "df_copy['Oxford'] = [True, False]\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since all we have is a single categorical feature here we can just read our likelihoods and priors right off of this table:\n",
    "\n",
    "Likelihoods:\n",
    "\n",
    "- Age45-60:\n",
    "    - P(Age45-60 | Oxford=True) = $\\frac{90}{350} = 0.2571$;\n",
    "    - P(Age45-60 | Oxford=False) = $\\frac{96}{268} = 0.3582$.\n",
    "\n",
    "Priors:\n",
    "\n",
    "- P(Oxford=True) = $\\frac{350}{618} = 0.5663$;\n",
    "- P(Oxford=False) = $\\frac{268}{618} = 0.4337$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.156445Z",
     "start_time": "2022-08-16T17:15:34.142445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Likelihood for those with a preference for the Oxford comma: {90 / 350}\")\n",
    "print(f\"Likelihood for those with a preference for no Oxford comma: {96 / 268}\")\n",
    "print(f\"Size of training data: {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.171945Z",
     "start_time": "2022-08-16T17:15:34.157445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Prior for those with a preference for the Oxford comma: {350 / 618}\")\n",
    "print(f\"Prior for those with a preference for no Oxford comma: {268 / 618}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we'll calculate the probability of the evidence:\n",
    "\n",
    "$$\\begin{align} \n",
    "    P(Age45-60) &= P(Age45-60 | Oxford=True) \\times P(Oxford=True) \\\\\n",
    "                & \\hspace{1cm} + P(Age45-60 | Oxford=False) \\times P(Oxford=False)\\\\ \n",
    "                &= 0.2571 \\times 0.5663 + 0.3582 \\times 0.4337 \\\\\n",
    "                &= 0.3010\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.187476Z",
     "start_time": "2022-08-16T17:15:34.173447Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This calculation should also yield P(e):\n",
    "# It's the proportion of 45-60-yr.-olds\n",
    "# in the data.\n",
    "\n",
    "90 / 618 + 96 / 618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now use Bayes's Theorem to calculate the posteriors:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(Oxford=True | Age45-60) &= \\frac{P(Oxford=True) \\times P(Age45-60 | Oxford=True)}{P(Age45-60)} \\\\\n",
    "                          &= \\frac{0.5663 \\times 0.2571}{0.3010} \\\\\n",
    "                          &= 0.4839 \\\\\n",
    "                          \\\\\n",
    "P(Oxford=False | Age45-60) &= \\frac{P(Oxford=False) \\times P(Age45-60 | Oxford=False)}{P(Age45-60)} \\\\ \n",
    "                          &= \\frac{0.4337 \\times 0.3582}{0.3010} \\\\\n",
    "                          &= 0.5161\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.202945Z",
     "start_time": "2022-08-16T17:15:34.188946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print((90/618) / (90/618 +  96/618))\n",
    "print((96/618) / (90/618 + 96/618))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Close! But our prediction for someone in the 45-60 age group will be that they **do not** favor the Oxford comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparison with [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.218472Z",
     "start_time": "2022-08-16T17:15:34.204445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model = MultinomialNB()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_coded = ohe.transform(X_train.values.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.233967Z",
     "start_time": "2022-08-16T17:15:34.219948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.fit(X_train_coded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.249474Z",
     "start_time": "2022-08-16T17:15:34.235445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_coded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Which age group is which?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.264977Z",
     "start_time": "2022-08-16T17:15:34.250946Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.280445Z",
     "start_time": "2022-08-16T17:15:34.274445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.predict_proba(np.array([0, 0, 1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Which comma-group is which?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:15:34.295979Z",
     "start_time": "2022-08-16T17:15:34.284445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These results agree very closely with ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
